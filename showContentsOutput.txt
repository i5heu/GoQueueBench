## /bench.sh
go test -bench=. ./cmd/bench
## /test.sh
go test -v -count=1  ./cmd/bench
## /cmd/markdownTable/main.go
package main

import (
	"encoding/json"
	"flag"
	"fmt"
	"os"
	"sort"
	"time"
)

// BenchmarkResult holds one benchmark result from test-results.json.
type BenchmarkResult struct {
	Implementation string  `json:"implementation"`
	NumMessages    int     `json:"num_messages"`
	NumProducers   int     `json:"num_producers"`
	NumConsumers   int     `json:"num_consumers"`
	AvgTime        string  `json:"avg_time"`
	AvgTimePerMsg  string  `json:"avg_time_per_msg"` // e.g. "197ns", "14.5µs"
	Throughput     float64 `json:"throughput_msgs_sec"`
	// Other fields omitted for brevity...
}

// FullReport represents one test session in test-results.json.
type FullReport struct {
	SessionTime string            `json:"session_time"`
	Benchmarks  []BenchmarkResult `json:"benchmarks"`
}

// implMetadata holds metadata for each implementation.
type implMetadata struct {
	pkgName  string
	features string
	author   string
}

// Hard-coded map from Implementation name → (package name, features, author).
var metadataMap = map[string]implMetadata{
	"Golang Buffered Channel": {
		pkgName:  "buffered",
		features: "MPMC, FIFO",
		author:   "[Mia Heidenstedt](http://heidenstedt.org)",
	},
	"BasicMPMCQueue - MPMC/FIFO": {
		pkgName:  "basicmpmc",
		features: "MPMC, FIFO",
		author:   "[Mia Heidenstedt](http://heidenstedt.org)",
	},
	"OptimizedMPMCQueue - MPMC/FIFO": {
		pkgName:  "optmpmc",
		features: "MPMC, FIFO",
		author:   "[Mia Heidenstedt](http://heidenstedt.org)",
	},
	"OptimizedMPMCQueueSharded - MPMC/Sharded": {
		pkgName:  "optmpmc_sharded",
		features: "MPMC, Sharded",
		author:   "[Mia Heidenstedt](http://heidenstedt.org)",
	},
}

// tableRow holds the data for one row in the Markdown table.
type tableRow struct {
	implementation string
	pkgName        string
	features       string
	author         string
	nsPerMsg       float64 // average execution time per message (ns)
}

func main() {
	// Command-line flags.
	jsonFile := flag.String("jsonfile", "test-results.json", "Path to JSON file containing test sessions")
	flag.Parse()

	// Load the JSON file.
	data, err := os.ReadFile(*jsonFile)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error reading JSON file %q: %v\n", *jsonFile, err)
		os.Exit(1)
	}

	// Parse the JSON data.
	var sessions []FullReport
	if err := json.Unmarshal(data, &sessions); err != nil {
		fmt.Fprintf(os.Stderr, "Error unmarshalling JSON: %v\n", err)
		os.Exit(1)
	}

	if len(sessions) == 0 {
		fmt.Fprintln(os.Stderr, "No sessions found in JSON.")
		os.Exit(1)
	}

	// Use the last session.
	lastSession := sessions[len(sessions)-1]

	// For each implementation, take the last benchmark encountered.
	lastBenchByImpl := make(map[string]BenchmarkResult)
	for _, bench := range lastSession.Benchmarks {
		lastBenchByImpl[bench.Implementation] = bench
	}

	// Convert each benchmark into a tableRow.
	var rows []tableRow
	for impl, bench := range lastBenchByImpl {
		// Parse the AvgTimePerMsg field (e.g. "197ns", "14.5µs").
		dur, err := time.ParseDuration(bench.AvgTimePerMsg)
		if err != nil {
			fmt.Fprintf(os.Stderr, "Error parsing AvgTimePerMsg '%s': %v\n", bench.AvgTimePerMsg, err)
			continue
		}
		nsValue := float64(dur.Nanoseconds())
		meta := metadataMap[impl] // May be empty if not defined.

		rows = append(rows, tableRow{
			implementation: impl,
			pkgName:        meta.pkgName,
			features:       meta.features,
			author:         meta.author,
			nsPerMsg:       nsValue,
		})
	}

	// Sort rows by average time per message (ns) ascending.
	sort.Slice(rows, func(i, j int) bool {
		return rows[i].nsPerMsg < rows[j].nsPerMsg
	})

	// Print Markdown table header.
	fmt.Println("## Last Session Benchmark Summary (sorted by Avg Time per Msg in ns)")
	fmt.Println()
	fmt.Println("| Implementation                               | Package            | Features         | Author                                     | Avg Time per Msg (ns) |")
	fmt.Println("|----------------------------------------------|--------------------|------------------|--------------------------------------------|-----------------------|")

	// Print each row.
	for _, r := range rows {
		fmt.Printf("| %-44s | %-18s | %-16s | %-42s | %21.0f |\n",
			r.implementation,
			r.pkgName,
			r.features,
			r.author,
			r.nsPerMsg,
		)
	}
}

## /cmd/bench/main_test.go
package main

import (
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/i5heu/GoQueueBench/pkg/queue"
)

// Pointer is a constraint that ensures T is always a pointer type.
type Pointer[T any] interface {
	*T
}

// Compile-time enforcement that T must be a pointer.
func enforcePointer[T any, PT interface{ ~*T }](q queue.MPMCQueue[PT]) {}

// progressWatchdog monitors progress and fails the test if no progress is made for 15 seconds.
type progressWatchdog struct {
	t            *testing.T
	label        string
	mu           sync.Mutex
	lastProgress time.Time
	done         chan struct{}
}

func newWatchdog(t *testing.T, label string) *progressWatchdog {
	return &progressWatchdog{
		t:            t,
		label:        label,
		lastProgress: time.Now(),
		done:         make(chan struct{}),
	}
}

func (wd *progressWatchdog) Start() {
	go func() {
		ticker := time.NewTicker(500 * time.Millisecond)
		defer ticker.Stop()
		for {
			select {
			case <-ticker.C:
				wd.mu.Lock()
				elapsed := time.Since(wd.lastProgress)
				wd.mu.Unlock()
				if elapsed > 15*time.Second {
					wd.t.Fatalf("No progress in the last 15 seconds (%s test likely stuck).", wd.label)
				}
			case <-wd.done:
				return
			}
		}
	}()
}

func (wd *progressWatchdog) Progress() {
	wd.mu.Lock()
	wd.lastProgress = time.Now()
	wd.mu.Unlock()
}

func (wd *progressWatchdog) Stop() {
	close(wd.done)
}

type testQueueInterface = interface {
	Enqueue(*int)
	Dequeue() (*int, bool)
	FreeSlots() uint64
	UsedSlots() uint64
}

// withAllQueues is a test helper that loops over all implementations
// and calls your test function for each one.
func withAllQueues(t *testing.T, scenarioName string, testedFeatures []string, fn func(t *testing.T, impl Implementation[*int, testQueueInterface])) {
	t.Helper()
	impls := getImplementations()
	for _, impl := range impls {
		impl := impl // capture range variable

		// check if the test tests a feature that the implementation does not support, if nil it will be tested
		if testedFeatures != nil {
			for _, feature := range testedFeatures {
				found := false
				for _, implFeature := range impl.features {
					if feature == implFeature {
						found = true
						break
					}
				}
				if !found {
					t.Skipf("Skipping test %q for implementation %q: missing feature %q \n", scenarioName, impl.name, feature)
				}
			}
		}

		t.Run(impl.name, func(t *testing.T) {
			if impl.newQueue == nil {
				t.Skipf("Skipping stub implementation %q", impl.name)
				return
			}
			fn(t, impl)
		})
	}
}

func TestBasicFIFO(t *testing.T) {
	withAllQueues(t, "BasicFIFO", []string{"FIFO"}, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		q := impl.newQueue(1024)

		wd := newWatchdog(t, "BasicFIFO")
		wd.Start()
		defer wd.Stop()

		const N = 1024

		// Enqueue N items, each carrying its sequence number.
		for i := 0; i < N; i++ {
			item := i
			q.Enqueue(&item) // Blocks if full
			wd.Progress()
		}

		// Dequeue N items, in FIFO order. Because Dequeue returns nil if empty,
		// we busy-wait until we get a value.
		for i := 0; i < N; i++ {
			var valPtr *int
			for {
				var ok bool
				valPtr, ok = q.Dequeue()
				if ok {
					break
				}
				time.Sleep(1 * time.Microsecond)
			}
			wd.Progress()
			if *valPtr != i {
				t.Fatalf("Expected %d, got %d at index %d", i, *valPtr, i)
			}
		}
	})
}

func TestHighContention(t *testing.T) {
	withAllQueues(t, "HighContention", []string{"MPMC", "FIFO"}, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		q := impl.newQueue(1024)

		wd := newWatchdog(t, "HighContention")
		wd.Start()
		defer wd.Stop()

		const (
			numProducers        = 500
			numConsumers        = 500
			messagesPerProducer = 10000
		)
		totalMessages := numProducers * messagesPerProducer

		sentCount := atomic.Uint64{}
		receivedCount := atomic.Uint64{}

		// Start producers.
		var prodWg sync.WaitGroup
		prodWg.Add(numProducers)
		for i := 0; i < numProducers; i++ {
			go func(prodID int) {
				defer prodWg.Done()
				for j := 0; j < messagesPerProducer; j++ {
					val := prodID + j
					q.Enqueue(&val) // blocks if full
					wd.Progress()
					sentCount.Add(1)
				}
			}(i)
		}

		// Divide the consumption workload among consumers.
		messagesPerConsumer := totalMessages / numConsumers
		remainder := totalMessages % numConsumers

		var consWg sync.WaitGroup
		consWg.Add(numConsumers)
		for i := 0; i < numConsumers; i++ {
			count := messagesPerConsumer
			if i == numConsumers-1 {
				count += remainder
			}
			go func(consumerID, count int) {
				defer consWg.Done()
				for j := 0; j < count; j++ {
					// Because Dequeue returns nil if empty, we busy-wait until we get a real value.
					for {
						_, ok := q.Dequeue()
						if ok {
							break
						}
						time.Sleep(1 * time.Microsecond)
					}
					wd.Progress()
					receivedCount.Add(1)
				}
			}(i, count)
		}

		// Wait for all producers and consumers.
		prodWg.Wait()
		consWg.Wait()

		if sentCount.Load() != uint64(totalMessages) {
			t.Fatalf("Expected to send %d messages, but sent %d", totalMessages, sentCount.Load())
		}
		if receivedCount.Load() != uint64(totalMessages) {
			t.Fatalf("Expected to receive %d messages, but received %d", totalMessages, receivedCount.Load())
		}
	})
}

func TestEmptyQueue(t *testing.T) {
	withAllQueues(t, "EmptyQueue", nil, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		q := impl.newQueue(1024)

		wd := newWatchdog(t, "EmptyQueue")
		wd.Start()
		defer wd.Stop()

		// If the queue is empty, Dequeue should return nil immediately (non-blocking).
		val, _ := q.Dequeue()
		if val != nil {
			t.Fatalf("Expected Dequeue to return nil on empty queue, got %v", val)
		}
		wd.Progress()

		// Enqueue an element.
		x := 42
		q.Enqueue(&x)
		wd.Progress()

		// Now Dequeue should yield the element.
		val, _ = q.Dequeue()
		if val == nil {
			t.Fatal("Expected to dequeue a valid pointer, got nil")
		}
		if *val != 42 {
			t.Fatalf("Expected to dequeue 42, got %v", *val)
		}
	})
}

func TestWrapAround(t *testing.T) {
	withAllQueues(t, "WrapAround", nil, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		q := impl.newQueue(1024)

		wd := newWatchdog(t, "WrapAround")
		wd.Start()
		defer wd.Stop()

		const capacity = 1024

		// Fill fully.
		for i := 0; i < capacity; i++ {
			val := i
			q.Enqueue(&val)
			wd.Progress()
		}
		// Dequeue half.
		for i := 0; i < capacity/2; i++ {
			var val *int
			for {
				val, _ = q.Dequeue()
				if val != nil {
					break
				}
				time.Sleep(1 * time.Microsecond)
			}
			wd.Progress()
		}
		// Enqueue again to force wrap-around.
		for i := 0; i < capacity/2; i++ {
			val := 1000 + i
			q.Enqueue(&val)
			wd.Progress()
		}
		// Dequeue everything and verify.
		for i := 0; i < capacity; i++ {
			var val *int
			for {
				val, _ = q.Dequeue()
				if val != nil {
					break
				}
				time.Sleep(1 * time.Microsecond)
			}
			wd.Progress()
		}
	})
}

func TestSmallStress(t *testing.T) {
	withAllQueues(t, "SmallStress", []string{"MPMC"}, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		q := impl.newQueue(1024)

		wd := newWatchdog(t, "SmallStress")
		wd.Start()
		defer wd.Stop()

		const (
			numProducers        = 2
			numConsumers        = 2
			messagesPerProducer = 2500
		)
		totalMessages := numProducers * messagesPerProducer

		sentCount := atomic.Uint64{}
		receivedCount := atomic.Uint64{}

		var prodWg sync.WaitGroup
		prodWg.Add(numProducers)
		for i := 0; i < numProducers; i++ {
			go func(prodID int) {
				defer prodWg.Done()
				for j := 0; j < messagesPerProducer; j++ {
					val := prodID*messagesPerProducer + j
					q.Enqueue(&val) // blocks if full
					wd.Progress()
					sentCount.Add(1)
				}
			}(i)
		}

		var consWg sync.WaitGroup
		consWg.Add(numConsumers)
		for i := 0; i < numConsumers; i++ {
			go func() {
				defer consWg.Done()
				for {
					// If we've received everything, stop.
					if receivedCount.Load() >= uint64(totalMessages) {
						return
					}
					// Because Dequeue can return nil, we busy-wait until a real value arrives.
					item, _ := q.Dequeue()
					if item != nil {
						receivedCount.Add(1)
						wd.Progress()
					} else {
						time.Sleep(1 * time.Millisecond)
					}
				}
			}()
		}

		prodWg.Wait()
		consWg.Wait()

		if sentCount.Load() != uint64(totalMessages) {
			t.Fatalf("Expected to send %d messages, but sent %d", totalMessages, sentCount.Load())
		}
		if receivedCount.Load() != uint64(totalMessages) {
			t.Fatalf("Expected to receive %d messages, but received %d", totalMessages, receivedCount.Load())
		}
	})
}

func TestUsedFreeSlots(t *testing.T) {
	withAllQueues(t, "UsedFreeSlots", nil, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		q := impl.newQueue(1024) // Assume internally it has some fixed capacity (e.g. 1024)

		wd := newWatchdog(t, "UsedFreeSlots")
		wd.Start()
		defer wd.Stop()

		// 1. Right after creation, we expect UsedSlots = 0, FreeSlots > 0.
		if q.UsedSlots() != 0 {
			t.Fatalf("Expected UsedSlots=0, got %d", q.UsedSlots())
		}
		if q.FreeSlots() == 0 {
			t.Fatalf("Expected FreeSlots>0, got %d", q.FreeSlots())
		}

		// 2. Enqueue a few items
		numEnqueues := 10
		for i := 0; i < numEnqueues; i++ {
			val := i
			q.Enqueue(&val)
			wd.Progress()
		}
		if q.UsedSlots() != uint64(numEnqueues) {
			t.Fatalf("Expected UsedSlots=%d, got %d", numEnqueues, q.UsedSlots())
		}
		// We can check that freeSlots + usedSlots = capacity if your queue
		// enforces a known capacity. If not, omit this check or adapt it.

		// 3. Dequeue half
		toDequeue := numEnqueues / 2
		for i := 0; i < toDequeue; i++ {
			valPtr, _ := q.Dequeue()
			if valPtr == nil {
				t.Fatalf("Expected a non-nil item after enqueuing %d items", numEnqueues)
			}
			wd.Progress()
		}
		if q.UsedSlots() != uint64(numEnqueues-toDequeue) {
			t.Fatalf("Expected UsedSlots=%d after dequeuing %d items, got %d",
				numEnqueues-toDequeue, toDequeue, q.UsedSlots())
		}
	})
}

func TestFullQueueBlocking(t *testing.T) {
	withAllQueues(t, "FullQueueBlocking", nil, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		const capacity = 1024 // If your queue uses a different default capacity, adapt as needed.
		q := impl.newQueue(1024)

		wd := newWatchdog(t, "FullQueueBlocking")
		wd.Start()
		defer wd.Stop()

		// Fill the queue exactly to capacity.
		for i := 0; i < capacity; i++ {
			x := i
			q.Enqueue(&x)
			wd.Progress()
		}

		if q.FreeSlots() != 0 {
			t.Fatalf("Expected FreeSlots=0 after enqueuing %d items, got %d", capacity, q.FreeSlots())
		}
		if q.UsedSlots() != uint64(capacity) {
			t.Fatalf("Expected UsedSlots=%d, got %d", capacity, q.UsedSlots())
		}

		blocked := make(chan struct{})
		done := make(chan struct{})

		go func() {
			defer close(done)
			val := 9999
			q.Enqueue(&val) // This should block until we free a slot.
			wd.Progress()
		}()

		// Wait a short time to confirm goroutine is blocked.
		select {
		case <-done:
			t.Fatal("Expected Enqueue to block, but goroutine completed immediately")
		case <-time.After(100 * time.Millisecond):
			// It's likely blocked, so signal success here by sending on 'blocked'.
			close(blocked)
		}

		// Now free one slot by dequeuing.
		valPtr, _ := q.Dequeue()
		if valPtr == nil {
			t.Fatal("Expected a valid item from Dequeue")
		}
		wd.Progress()

		// Now the Enqueue goroutine should unblock and complete
		select {
		case <-done:
			// Good, it unblocked.
		case <-time.After(2 * time.Second):
			t.Fatal("Enqueue goroutine did not unblock after freeing a slot")
		}

		// Verify final usage count: we re-enqueued one after freeing a slot
		if q.UsedSlots() != uint64(capacity) {
			t.Fatalf("Expected queue to still be at capacity, got UsedSlots=%d", q.UsedSlots())
		}
	})
}

func TestMixedConcurrentOps(t *testing.T) {
	withAllQueues(t, "MixedConcurrentOps", []string{"MPMC"}, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		q := impl.newQueue(1024)

		wd := newWatchdog(t, "MixedConcurrentOps")
		wd.Start()
		defer wd.Stop()

		const (
			numGoroutines = 1000
			loopCount     = 1000
		)

		var wg sync.WaitGroup
		wg.Add(numGoroutines)

		for g := 0; g < numGoroutines; g++ {
			go func(gID int) {
				defer wg.Done()
				for i := 0; i < loopCount; i++ {
					// ENQUEUE
					val := (gID << 16) + i
					q.Enqueue(&val)
					wd.Progress()

					// DEQUEUE
					var got *int
					for {
						got, _ = q.Dequeue()
						if got != nil {
							break
						}
						time.Sleep(time.Microsecond)
					}
					wd.Progress()
				}
			}(g)
		}
		wg.Wait()

		// By design, each goroutine enqueues once and dequeues once in each iteration.
		// So at the end, the queue should end up empty.
		used := q.UsedSlots()
		if used != 0 {
			t.Fatalf("Expected queue to be empty (UsedSlots=0), got %d", used)
		}
	})
}

func TestNilEnqueue(t *testing.T) {
	withAllQueues(t, "NilEnqueue", nil, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		q := impl.newQueue(1024)
		wd := newWatchdog(t, "NilEnqueue")
		wd.Start()
		defer wd.Stop()

		// Enqueue a nil pointer.
		q.Enqueue(nil)
		wd.Progress()

		// Check that the queue counts the nil as an enqueued element.
		if q.UsedSlots() != 1 {
			t.Fatalf("Expected UsedSlots=1 after enqueuing nil, got %d", q.UsedSlots())
		}

		// Dequeue should return nil (which was enqueued).
		val, _ := q.Dequeue()
		if val != nil {
			t.Fatalf("Expected dequeued value to be nil when enqueued nil, got %v", val)
		}
		wd.Progress()

		// Now the queue should be empty.
		if q.UsedSlots() != 0 {
			t.Fatalf("Expected queue to be empty after dequeuing, got UsedSlots=%d", q.UsedSlots())
		}
	})
}

func TestRepeatedEmptyDequeue(t *testing.T) {
	withAllQueues(t, "RepeatedEmptyDequeue", nil, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		q := impl.newQueue(1024)
		wd := newWatchdog(t, "RepeatedEmptyDequeue")
		wd.Start()
		defer wd.Stop()

		for i := 0; i < 1000; i++ {
			val, _ := q.Dequeue()
			if val != nil {
				t.Fatalf("Expected nil from empty Dequeue at iteration %d", i)
			}
			wd.Progress()
		}
		if q.UsedSlots() != 0 {
			t.Fatalf("Expected queue to remain empty after repeated Dequeue calls, got %d", q.UsedSlots())
		}
	})
}

func TestHighWrapAround(t *testing.T) {
	withAllQueues(t, "HighWrapAround", nil, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		q := impl.newQueue(1024)
		wd := newWatchdog(t, "HighWrapAround")
		wd.Start()
		defer wd.Stop()

		const iterations = 1000000
		for i := 0; i < iterations; i++ {
			val := i
			q.Enqueue(&val)
			wd.Progress()
			item, _ := q.Dequeue()
			if item == nil {
				t.Fatalf("Expected valid item at iteration %d", i)
			}
			if *item != i {
				t.Fatalf("Expected %d, got %d at iteration %d", i, *item, i)
			}
			wd.Progress()
		}
		if q.UsedSlots() != 0 {
			t.Fatalf("Expected queue to be empty after high wrap-around test, got %d", q.UsedSlots())
		}
	})
}

func TestConcurrentUsageCounters(t *testing.T) {
	withAllQueues(t, "ConcurrentUsageCounters", []string{"MPMC"}, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		const capacity = 1024
		q := impl.newQueue(1024)
		wd := newWatchdog(t, "ConcurrentUsageCounters")
		wd.Start()
		defer wd.Stop()

		var wg sync.WaitGroup
		wg.Add(1)
		go func() {
			defer wg.Done()
			for i := 0; i < 100000; i++ {
				val := i
				q.Enqueue(&val)
				q.Dequeue()
				wd.Progress()
			}
		}()

		wg.Wait()

		// Concurrently verify that the sum of FreeSlots and UsedSlots equals capacity.

		used := q.UsedSlots()
		free := q.FreeSlots()
		if used+free != capacity {
			t.Fatalf("Usage counters inconsistent: UsedSlots(%d) + FreeSlots(%d) != %d", used, free, capacity)
		}
		wd.Progress()
		time.Sleep(10 * time.Microsecond)

	})
}

func TestAlternatingSingleCapacity(t *testing.T) {
	withAllQueues(t, "AlternatingSingleCapacity", nil, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		q := impl.newQueue(1)
		wd := newWatchdog(t, "AlternatingSingleCapacity")
		wd.Start()
		defer wd.Stop()

		const iterations = 1000000
		for i := 0; i < iterations; i++ {
			val := i
			q.Enqueue(&val)
			wd.Progress()
			item, _ := q.Dequeue()
			if item == nil {
				t.Fatalf("Expected valid item in iteration %d", i)
			}
			if *item != i {
				t.Fatalf("Expected %d, got %d at iteration %d", i, *item, i)
			}
			wd.Progress()
		}

		if q.UsedSlots() != 0 {
			t.Fatalf("Expected queue to be empty after alternating operations, got %d", q.UsedSlots())
		}
	})
}

func TestFIFOPointerIntegrity(t *testing.T) {
	withAllQueues(t, "PointerIntegrity", []string{"FIFO"}, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		q := impl.newQueue(1024)
		wd := newWatchdog(t, "PointerIntegrity")
		wd.Start()
		defer wd.Stop()

		const numItems = 100
		originalPointers := make([]*int, numItems)

		// Enqueue pointers to newly allocated ints with unique addresses and values.
		for i := 0; i < numItems; i++ {
			p := new(int)
			*p = i
			originalPointers[i] = p
			q.Enqueue(p)
			wd.Progress()
		}

		// Dequeue each item and verify that the pointer and its value are unchanged.
		for i := 0; i < numItems; i++ {
			var got *int
			for {
				got, _ = q.Dequeue()
				if got != nil {
					break
				}
				time.Sleep(1 * time.Microsecond)
			}
			wd.Progress()
			if got != originalPointers[i] {
				t.Fatalf("Pointer corruption at index %d: expected pointer %p, got %p", i, originalPointers[i], got)
			}
			if *got != i {
				t.Fatalf("Value corruption at index %d: expected %d, got %d", i, i, *got)
			}
		}
	})
}

func TestDetailedPointerIntegrityWrapAround(t *testing.T) {
	withAllQueues(t, "TestDetailedPointerIntegrityWrapAround", nil, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		// Use a small capacity to force wrap-around behavior.
		const smallCapacity = 64
		// totalOps is the number of enqueue operations performed by the writer.
		const totalOps = 2000000
		q := impl.newQueue(smallCapacity)

		wd := newWatchdog(t, "TestDetailedPointerIntegrityWrapAround")
		wd.Start()
		defer wd.Stop()

		// expectedChan holds the pointers in the exact order they were enqueued.
		// Its capacity is the total number of items expected (initial fill + writer ops).
		totalExpected := totalOps + smallCapacity
		expectedChan := make(chan *int, totalExpected)

		// Pre-fill: enqueue smallCapacity items with values 0..smallCapacity-1.
		for i := 0; i < smallCapacity; i++ {
			ptr := new(int)
			*ptr = i
			q.Enqueue(ptr)
			expectedChan <- ptr
			wd.Progress()
		}

		// Launch a writer goroutine that enqueues totalOps new items.
		doneWriter := make(chan struct{})
		go func() {
			// nextValue starts at smallCapacity so that the overall values form a continuous increasing sequence.
			nextValue := smallCapacity
			for op := 0; op < totalOps; op++ {
				newPtr := new(int)
				*newPtr = nextValue
				q.Enqueue(newPtr)
				expectedChan <- newPtr
				nextValue++
				wd.Progress()
			}
			close(doneWriter)
		}()

		// Now, in the main (reader) goroutine, perform totalExpected dequeue operations.
		// For each operation, wait until a pointer is available then compare it to the expected pointer.
		for op := 0; op < totalExpected; op++ {
			var got *int
			for {
				got, _ = q.Dequeue()
				if got != nil {
					break
				}
				time.Sleep(1 * time.Microsecond)
			}
			wd.Progress()
			expected := <-expectedChan
			// Verify that the pointer addresses match.
			if got != expected {
				t.Fatalf("Pointer mismatch at op %d: expected pointer %p, got %p", op, expected, got)
			}
			// Verify that the stored value matches the expected sequence.
			if *got != op {
				t.Fatalf("Value mismatch at op %d: expected %d, got %d", op, op, *got)
			}
		}

		// Wait for the writer goroutine to finish.
		<-doneWriter

		// Finally, the queue should be empty.
		if q.UsedSlots() != 0 {
			t.Fatalf("Expected queue to be empty after all operations, but UsedSlots = %d", q.UsedSlots())
		}
	})
}

func TestNonFIFOPointerIntegrity(t *testing.T) {
	withAllQueues(t, "NonFIFO", []string{"MPMC"}, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		const numItems = 100000
		q := impl.newQueue(256)
		wd := newWatchdog(t, "NonFIFO")
		wd.Start()
		defer wd.Stop()

		// expected will hold all pointers that are enqueued.
		// The boolean value will be set to true once the pointer is seen during dequeue.
		expected := make(map[*int]bool, numItems)
		expectedIn := make(map[*int]bool, numItems)
		for i := 0; i < numItems; i++ {
			p := new(int)
			*p = i
			expected[p] = false
			expectedIn[p] = false

			wd.Progress()
		}

		go func() {
			// Enqueue pointers
			for ptr := range expectedIn {
				q.Enqueue(ptr)
				wd.Progress()
			}
		}()

		// Dequeue until we've received exactly numItems elements.
		receivedCount := 0
		for receivedCount < numItems {
			var p *int
			var ok bool
			p, ok = q.Dequeue()
			if ok && p != nil {
				// Check that this pointer was indeed enqueued.
				if _, exists := expected[p]; !exists {
					t.Fatalf("Received pointer %p which was not enqueued", p)
				}
				// If the pointer has already been seen, then it's a duplicate.
				if expected[p] {
					t.Fatalf("Received pointer %p more than once", p)
				}
				expected[p] = true
				receivedCount++
			} else {
				// No value available; wait a moment.
				time.Sleep(1 * time.Microsecond)
			}
			wd.Progress()
		}

		// Verify that every enqueued pointer was seen exactly once.
		for p, seen := range expected {
			if !seen {
				t.Fatalf("Expected pointer %p was not received", p)
			}
		}
	})
}

func TestConcurrentNonFIFOMultiRW(t *testing.T) {
	withAllQueues(t, "ConcurrentNonFIFO", []string{"MPMC"}, func(t *testing.T, impl Implementation[*int, testQueueInterface]) {
		const (
			numProducers     = 1000
			numConsumers     = 1000
			itemsPerProducer = 1000
		)
		totalItems := numProducers * itemsPerProducer

		q := impl.newQueue(1024)
		wd := newWatchdog(t, "ConcurrentNonFIFO")
		wd.Start()
		defer wd.Stop()

		var producedCount atomic.Uint64
		var consumedCount atomic.Uint64

		// Start producers.
		var wgProducers sync.WaitGroup
		wgProducers.Add(numProducers)
		for p := 0; p < numProducers; p++ {
			go func(producerID int) {
				defer wgProducers.Done()
				for i := 0; i < itemsPerProducer; i++ {
					// Each produced value is unique.
					val := producerID*itemsPerProducer + i
					q.Enqueue(&val)
					producedCount.Add(1)
					wd.Progress()
				}
			}(p)
			wd.Progress()
		}

		// Start consumers.
		var wgConsumers sync.WaitGroup
		wgConsumers.Add(numConsumers)
		for c := 0; c < numConsumers; c++ {
			go func(consumerID int) {
				defer wgConsumers.Done()
				for {
					// Stop if we've consumed all items.
					if consumedCount.Load() >= uint64(totalItems) {
						return
					}
					ptr, ok := q.Dequeue()
					if ok && ptr != nil {
						consumedCount.Add(1)
						wd.Progress()
					} else {
						time.Sleep(1 * time.Microsecond)
					}
				}
			}(c)
		}

		// Wait for producers to finish.
		wgProducers.Wait()
		// Wait until the consumers have consumed all items.
		for consumedCount.Load() < uint64(totalItems) {
			time.Sleep(1 * time.Millisecond)
		}
		wgConsumers.Wait()

		if producedCount.Load() != uint64(totalItems) {
			t.Fatalf("Produced count mismatch: expected %d, got %d", totalItems, producedCount.Load())
		}
		if consumedCount.Load() != uint64(totalItems) {
			t.Fatalf("Consumed count mismatch: expected %d, got %d", totalItems, consumedCount.Load())
		}
	})
}

func BenchmarkEnqueueDequeue(b *testing.B) {
	impls := getImplementations()
	for _, impl := range impls {
		// Skip stub implementations.
		if impl.newQueue == nil {
			continue
		}
		b.Run(impl.name, func(b *testing.B) {
			q := impl.newQueue(1024)
			b.ResetTimer()
			for i := 0; i < b.N; i++ {
				x := i
				q.Enqueue(&x)
				// Busy-wait until a value is dequeued.
				for {
					if _, ok := q.Dequeue(); ok {
						break
					}
				}
			}
		})
	}
}

## /cmd/bench/main.go
package main

import (
	"encoding/json"
	"flag"
	"fmt"
	"os"
	"runtime"
	"sort"
	"time"

	"github.com/i5heu/GoQueueBench/pkg/basicmpmc"
	"github.com/i5heu/GoQueueBench/pkg/buffered"
	"github.com/i5heu/GoQueueBench/pkg/optmpmc"
	"github.com/i5heu/GoQueueBench/pkg/optmpmc_sharded"
	"github.com/i5heu/GoQueueBench/pkg/testbench"
	"github.com/shirou/gopsutil/v3/cpu"
	"github.com/shirou/gopsutil/v3/mem"
)

// BenchmarkResult holds the result for one benchmark run.
type BenchmarkResult struct {
	Implementation string  `json:"implementation"`
	NumMessages    int     `json:"num_messages"`
	NumProducers   int     `json:"num_producers"`
	NumConsumers   int     `json:"num_consumers"`
	AvgTime        string  `json:"avg_time"`            // formatted duration string
	Throughput     float64 `json:"throughput_msgs_sec"` // messages per second
	AvgTimePerMsg  string  `json:"avg_time_per_msg"`    // formatted duration string
	MinTime        string  `json:"min_time"`            // minimum duration observed
	MaxTime        string  `json:"max_time"`            // maximum duration observed
	MedianTime     string  `json:"median_time"`         // median duration observed
	Timestamp      int64   `json:"timestamp"`           // Unix time (seconds)
	GoVersion      string  `json:"go_version"`
}

// SystemInfo holds details about the system environment.
type SystemInfo struct {
	NumCPU      int     `json:"num_cpu"`
	CPUModel    string  `json:"cpu_model,omitempty"`
	CPUSpeedMHz float64 `json:"cpu_speed_mhz,omitempty"`
	GOARCH      string  `json:"go_arch"`
	TotalMemory uint64  `json:"total_memory_bytes,omitempty"`
}

// FullReport represents one test session containing system information and benchmark results.
type FullReport struct {
	SessionTime string            `json:"session_time"` // human-readable session start time
	SystemInfo  SystemInfo        `json:"system_info"`
	Benchmarks  []BenchmarkResult `json:"benchmarks"`
}

// Implementation is a convenience struct for enumerating queue implementations
// without a runtime interface. newQueue returns a queue that satisfies
// the compile-time constraint. We now include FreeSlots() and UsedSlots()
// so that each queue must implement these methods as well.
type Implementation[T any, Q interface {
	Enqueue(T)
	Dequeue() (T, bool)
	FreeSlots() uint64
	UsedSlots() uint64
}] struct {
	name        string
	description string
	pkgName     string
	authors     []string
	features    []string
	newQueue    func(capacity uint64) Q
}

func main() {
	// Command-line flag: set -json to export benchmark results as JSON.
	jsonExport := flag.Bool("json", false, "Export benchmark results as JSON and append to test-results.json")
	flag.Parse()

	// Gather system information.
	sysInfo := gatherSystemInfo()

	// Print system information.
	fmt.Println("System Information:")
	fmt.Printf("  Num CPU: %d\n", sysInfo.NumCPU)
	fmt.Printf("  CPU Model: %s\n", sysInfo.CPUModel)
	fmt.Printf("  CPU Speed (MHz): %.2f\n", sysInfo.CPUSpeedMHz)
	fmt.Printf("  Architecture: %s\n", sysInfo.GOARCH)
	fmt.Printf("  Total Memory (bytes): %d\n\n", sysInfo.TotalMemory)

	iterations := 8

	configs := []testbench.Config{
		{NumMessages: 100000, NumProducers: 10, NumConsumers: 10},
		{NumMessages: 100000, NumProducers: 50, NumConsumers: 50},
		{NumMessages: 100000, NumProducers: 100, NumConsumers: 100},
		{NumMessages: 100000, NumProducers: 250, NumConsumers: 250},
		{NumMessages: 100000, NumProducers: 500, NumConsumers: 500},
		{NumMessages: 100000, NumProducers: 1000, NumConsumers: 1000},
		// {NumMessages: 100000, NumProducers: 2500, NumConsumers: 2500},
		// {NumMessages: 100000, NumProducers: 5000, NumConsumers: 5000},
		// {NumMessages: 100000, NumProducers: 10000, NumConsumers: 10000},
	}

	// For this example, we use *int as the type parameter.
	valueGenerator := func(i int) *int {
		v := i
		return &v
	}

	impls := getImplementations()

	// Slice to hold benchmark results.
	var results []BenchmarkResult

	// Run benchmarks for each implementation.
	for _, impl := range impls {
		fmt.Println("Benchmarking", impl.name)
		for _, cfg := range configs {
			var total time.Duration
			var durations []time.Duration
			for i := 0; i < iterations; i++ {
				q := impl.newQueue(1024)
				duration := testbench.RunTest[*int](q, cfg, valueGenerator)
				total += duration
				durations = append(durations, duration)
			}

			avg := total / time.Duration(iterations)
			mps := float64(cfg.NumMessages) / avg.Seconds()
			avgPerMsg := avg / time.Duration(cfg.NumMessages)
			minTime, maxTime, medianTime := computeMinMaxMedian(durations)
			timestamp := time.Now().Unix()
			goVersion := runtime.Version()

			// Print human-readable output.
			fmt.Printf("  Config: %d messages, %d producers, %d consumers:\n", cfg.NumMessages, cfg.NumProducers, cfg.NumConsumers)
			fmt.Printf("    Avg time = %v\n", avg)
			fmt.Printf("    Throughput = %.2f msgs/sec\n", mps)
			fmt.Printf("    Avg time per msg = %v\n", avgPerMsg)
			fmt.Printf("    Min time = %v, Max time = %v, Median time = %v\n", minTime, maxTime, medianTime)
			fmt.Printf("    Timestamp = %d, Go Version = %s\n", timestamp, goVersion)

			// Store result.
			result := BenchmarkResult{
				Implementation: impl.name,
				NumMessages:    cfg.NumMessages,
				NumProducers:   cfg.NumProducers,
				NumConsumers:   cfg.NumConsumers,
				AvgTime:        avg.String(),
				Throughput:     mps,
				AvgTimePerMsg:  avgPerMsg.String(),
				MinTime:        minTime.String(),
				MaxTime:        maxTime.String(),
				MedianTime:     medianTime.String(),
				Timestamp:      timestamp,
				GoVersion:      goVersion,
			}
			results = append(results, result)
		}
		fmt.Println()
	}

	// If the JSON export flag is set, append this test session to the file.
	if *jsonExport {
		sessionTime := time.Now().Format(time.RFC3339)
		fullReport := FullReport{
			SessionTime: sessionTime,
			SystemInfo:  sysInfo,
			Benchmarks:  results,
		}

		const filename = "test-results.json"
		var sessions []FullReport

		// Check if the file exists and load previous sessions.
		if _, err := os.Stat(filename); err == nil {
			data, err := os.ReadFile(filename)
			if err != nil {
				fmt.Fprintln(os.Stderr, "Error reading file:", err)
				os.Exit(1)
			}
			if len(data) > 0 {
				if err = json.Unmarshal(data, &sessions); err != nil {
					fmt.Fprintln(os.Stderr, "Error unmarshalling JSON:", err)
					os.Exit(1)
				}
			}
		} else if !os.IsNotExist(err) {
			// Some other error occurred
			fmt.Fprintln(os.Stderr, "Error stating file:", err)
			os.Exit(1)
		}

		// Append the new test session.
		sessions = append(sessions, fullReport)

		// Write back to the file.
		data, err := json.MarshalIndent(sessions, "", "  ")
		if err != nil {
			fmt.Fprintln(os.Stderr, "Error marshalling JSON:", err)
			os.Exit(1)
		}

		if err = os.WriteFile(filename, data, 0644); err != nil {
			fmt.Fprintln(os.Stderr, "Error writing file:", err)
			os.Exit(1)
		}

		fmt.Println("Results appended to", filename)
	}
}

// computeMinMaxMedian computes the minimum, maximum, and median durations from a slice.
func computeMinMaxMedian(durations []time.Duration) (min, max, median time.Duration) {
	if len(durations) == 0 {
		return 0, 0, 0
	}

	min = durations[0]
	max = durations[0]
	for _, d := range durations {
		if d < min {
			min = d
		}
		if d > max {
			max = d
		}
	}

	// Create a copy and sort it to compute the median.
	sortedDurations := make([]time.Duration, len(durations))
	copy(sortedDurations, durations)
	sort.Slice(sortedDurations, func(i, j int) bool { return sortedDurations[i] < sortedDurations[j] })
	median = sortedDurations[len(sortedDurations)/2]
	return
}

// gatherSystemInfo collects CPU and memory information using gopsutil and runtime.
func gatherSystemInfo() SystemInfo {
	numCPU := runtime.NumCPU()
	goArch := runtime.GOARCH

	// Get CPU info.
	var cpuModel string
	var cpuSpeed float64
	if infos, err := cpu.Info(); err == nil && len(infos) > 0 {
		cpuModel = infos[0].ModelName
		cpuSpeed = infos[0].Mhz
	}

	// Get memory info.
	var totalMemory uint64
	if vm, err := mem.VirtualMemory(); err == nil {
		totalMemory = vm.Total
	}

	return SystemInfo{
		NumCPU:      numCPU,
		CPUModel:    cpuModel,
		CPUSpeedMHz: cpuSpeed,
		GOARCH:      goArch,
		TotalMemory: totalMemory,
	}
}

// getImplementations enumerates our different queue implementations.
// Notice each one must also implement FreeSlots() and UsedSlots()
// so the type constraint is satisfied.
func getImplementations() []Implementation[*int, interface {
	Enqueue(*int)
	Dequeue() (*int, bool)
	FreeSlots() uint64
	UsedSlots() uint64
}] {
	return []Implementation[*int, interface {
		Enqueue(*int)
		Dequeue() (*int, bool)
		FreeSlots() uint64
		UsedSlots() uint64
	}]{
		{
			name:        "Golang Buffered Channel",
			pkgName:     "buffered",
			description: "Works with misused standard go channels, it would be much faster if the go routines would not switch a lot like in this test.",
			authors:     []string{"Mia Heidenstedt <heidenstedt.org>"},
			features:    []string{"MPMC", "FIFO"},
			newQueue: func(capacity uint64) interface {
				Enqueue(*int)
				Dequeue() (*int, bool)
				FreeSlots() uint64
				UsedSlots() uint64
			} {
				return buffered.New[*int](capacity)
			},
		},
		{
			name:        "BasicMPMCQueue",
			pkgName:     "basicmpmc",
			description: "A basic MPMC queue with no optimizations.",
			authors:     []string{"Mia Heidenstedt <heidenstedt.org>"},
			features:    []string{"MPMC", "FIFO"},
			newQueue: func(capacity uint64) interface {
				Enqueue(*int)
				Dequeue() (*int, bool)
				FreeSlots() uint64
				UsedSlots() uint64
			} {
				return basicmpmc.New[*int](capacity)
			},
		},
		{
			name:        "OptimizedMPMCQueue",
			pkgName:     "optmpmc",
			description: "An optimized MPMC queue with padding to reduce false sharing.",
			authors:     []string{"Mia Heidenstedt <heidenstedt.org>"},
			features:    []string{"MPMC", "FIFO"},
			newQueue: func(capacity uint64) interface {
				Enqueue(*int)
				Dequeue() (*int, bool)
				FreeSlots() uint64
				UsedSlots() uint64
			} {
				return optmpmc.New[*int](capacity)
			},
		},
		{
			name:        "OptimizedMPMCQueueSharded",
			pkgName:     "optmpmc_sharded",
			description: "An optimized MPMC queue with sharding to reduce contention.",
			authors:     []string{"Mia Heidenstedt <heidenstedt.org>"},
			features:    []string{"MPMC", "Sharded", "Multi-Head-FIFO"},
			newQueue: func(capacity uint64) interface {
				Enqueue(*int)
				Dequeue() (*int, bool)
				FreeSlots() uint64
				UsedSlots() uint64
			} {
				return optmpmc_sharded.New[*int](capacity)
			},
		},
	}
}

## /cmd/buildGraph/main.go
package main

import (
	"encoding/json"
	"flag"
	"fmt"
	"image/color"
	"os"
	"sort"
	"time"

	"gonum.org/v1/plot"
	"gonum.org/v1/plot/plotter"
	"gonum.org/v1/plot/plotutil"
	"gonum.org/v1/plot/vg"
)

// BenchmarkResult holds one benchmark result.
type BenchmarkResult struct {
	Implementation string `json:"implementation"`
	NumMessages    int    `json:"num_messages"`
	NumProducers   int    `json:"num_producers"`
	NumConsumers   int    `json:"num_consumers"`
	AvgTime        string `json:"avg_time"`
	// Other fields omitted for brevity...
}

// FullReport represents one test session.
type FullReport struct {
	SessionTime string            `json:"session_time"` // human-readable session start time
	Benchmarks  []BenchmarkResult `json:"benchmarks"`
}

func main() {
	// Optional flags
	jsonFile := flag.String("jsonfile", "test-results.json", "Path to JSON file containing test sessions")
	output := flag.String("out", "benchmark_graph.png", "Output graph image filename")
	flag.Parse()

	// Load all test sessions from JSON.
	data, err := os.ReadFile(*jsonFile)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error reading JSON file: %v\n", err)
		os.Exit(1)
	}

	var sessions []FullReport
	if err := json.Unmarshal(data, &sessions); err != nil {
		fmt.Fprintf(os.Stderr, "Error unmarshalling JSON: %v\n", err)
		os.Exit(1)
	}

	// Group data: for each Implementation, we collect (X,Y) points:
	//   X = (NumProducers + NumConsumers)
	//   Y = average time per run in nanoseconds (dur.Nanoseconds()) / NumMessages
	pointsByImpl := make(map[string]plotter.XYs)

	for _, session := range sessions {
		for _, res := range session.Benchmarks {
			x := float64(res.NumProducers + res.NumConsumers)

			dur, err := time.ParseDuration(res.AvgTime)
			if err != nil {
				fmt.Fprintf(os.Stderr, "Error parsing AvgTime '%s': %v\n", res.AvgTime, err)
				continue
			}

			// Convert total average time to per-message time in nanoseconds.
			nsPerMsg := float64(dur.Nanoseconds()) / float64(res.NumMessages)

			pointsByImpl[res.Implementation] = append(pointsByImpl[res.Implementation],
				plotter.XY{X: x, Y: nsPerMsg})
		}
	}

	// Sort each implementation's points by X so lines don't zigzag.
	for _, pts := range pointsByImpl {
		sort.Slice(pts, func(i, j int) bool {
			return pts[i].X < pts[j].X
		})
	}

	// Create the plot.
	p := plot.New()
	p.Title.Text = "Benchmark Average Time per Msg vs. (Producers + Consumers)"
	p.X.Label.Text = "NumProducers + NumConsumers"
	p.Y.Label.Text = "Avg Time per Msg (ns) [log scale]"

	//------------------------------------------------
	// Dark Theme
	//------------------------------------------------
	darkGray := color.RGBA{R: 30, G: 30, B: 30, A: 255}
	white := color.RGBA{R: 255, G: 255, B: 255, A: 255}

	p.BackgroundColor = darkGray
	// p.Canvas.Color = darkGray

	p.Title.TextStyle.Color = white
	p.X.Label.TextStyle.Color = white
	p.Y.Label.TextStyle.Color = white

	p.X.Color = white
	p.Y.Color = white
	p.X.Tick.Label.Color = white
	p.Y.Tick.Label.Color = white
	p.X.Tick.LineStyle.Color = white
	p.Y.Tick.LineStyle.Color = white

	//------------------------------------------------
	// Legend: top, horizontal
	//------------------------------------------------
	p.Legend.Top = true // place legend at the top
	p.Legend.Left = true
	p.Legend.TextStyle.Color = white
	p.Legend.ThumbnailWidth = vg.Points(8)

	//------------------------------------------------
	// Log Scale on Y-axis with custom ticker
	//------------------------------------------------
	p.Y.Scale = plot.LogScale{}
	p.Y.Tick.Marker = customNsLogTicks{}

	// Build line data for each implementation.
	var items []interface{}
	for impl, pts := range pointsByImpl {
		items = append(items, impl, pts)
	}

	// Add lines to the plot.
	if err := plotutil.AddLinePoints(p, items...); err != nil {
		fmt.Fprintf(os.Stderr, "Error adding line points: %v\n", err)
		os.Exit(1)
	}

	// Save the plot to a PNG file (large for detail).
	if err := p.Save(12*vg.Inch, 9*vg.Inch, *output); err != nil {
		fmt.Fprintf(os.Stderr, "Error saving plot: %v\n", err)
		os.Exit(1)
	}
	fmt.Printf("Graph saved to %s\n", *output)
}

// customNsLogTicks wraps the built-in log ticks, but formats them as "XXXns", "XXXµs", "XXXms", or "XXXs".
type customNsLogTicks struct{}

func (customNsLogTicks) Ticks(min, max float64) []plot.Tick {
	// Use default log ticks first.
	ticks := plot.LogTicks{}.Ticks(min, max)

	// For each tick, replace the label with a time-friendly label.
	for i := range ticks {
		if ticks[i].Label == "" {
			// Minor tick (no label).
			continue
		}
		val := ticks[i].Value
		ticks[i].Label = formatNs(val)
	}
	return ticks
}

// formatNs returns a string like "300ns", "1.2µs", "2.5ms", "1.0s", etc.
func formatNs(ns float64) string {
	switch {
	case ns < 1e3:
		// < 1,000 ns
		return fmt.Sprintf("%.0fns", ns)
	case ns < 1e6:
		// < 1,000,000 ns => microseconds
		return fmt.Sprintf("%.1fµs", ns/1e3)
	case ns < 1e9:
		// < 1,000,000,000 ns => milliseconds
		return fmt.Sprintf("%.1fms", ns/1e6)
	default:
		// >= 1e9 ns => seconds
		return fmt.Sprintf("%.2fs", ns/1e9)
	}
}

## /pkg/basicmpmc/basicmpmc.go
package basicmpmc

import (
	"runtime"
	"sync/atomic"
)

// basicCell represents one slot in the ring buffer.
type basicCell[T any] struct {
	sequence uint64
	value    T
}

// BasicMPMCQueue is a bounded, lock‑free, multi‑producer/multi‑consumer queue.
type BasicMPMCQueue[T any] struct {
	buffer     []basicCell[T]
	mask       uint64
	capacity   uint64
	enqueuePos uint64
	dequeuePos uint64
}

// New creates a new BasicMPMCQueue with the given capacity (rounded up to a power of 2).
func New[T any](capacity uint64) *BasicMPMCQueue[T] {
	if capacity&(capacity-1) != 0 {
		capPow := uint64(1)
		for capPow < capacity {
			capPow <<= 1
		}
		capacity = capPow
	}
	q := &BasicMPMCQueue[T]{
		buffer:   make([]basicCell[T], capacity),
		mask:     capacity - 1,
		capacity: capacity,
	}
	for i := uint64(0); i < capacity; i++ {
		q.buffer[i].sequence = i
	}
	return q
}

// Enqueue inserts a value into the queue.
// It spins until a slot is available.
func (q *BasicMPMCQueue[T]) Enqueue(val T) {
	for {
		pos := atomic.LoadUint64(&q.enqueuePos)
		cell := &q.buffer[pos&q.mask]
		seq := atomic.LoadUint64(&cell.sequence)
		// If the cell is free to write (expected sequence equals pos)
		if int64(seq)-int64(pos) == 0 {
			if atomic.CompareAndSwapUint64(&q.enqueuePos, pos, pos+1) {
				cell.value = val
				atomic.StoreUint64(&cell.sequence, pos+1)
				return
			}
		} else {
			runtime.Gosched()
		}
	}
}

// Dequeue removes and returns a value from the queue.
// It spins until a cell is ready.
func (q *BasicMPMCQueue[T]) Dequeue() (T, bool) {
	pos := atomic.LoadUint64(&q.dequeuePos)
	cell := &q.buffer[pos&q.mask]
	seq := atomic.LoadUint64(&cell.sequence)
	// If the cell is full (expected sequence equals pos+1)
	if int64(seq)-int64(pos+1) == 0 {
		if atomic.CompareAndSwapUint64(&q.dequeuePos, pos, pos+1) {
			ret := cell.value
			// Mark the cell as free by setting sequence to pos + capacity
			atomic.StoreUint64(&cell.sequence, pos+q.capacity)
			return ret, true
		}
	} else {
		runtime.Gosched()
	}

	var zero T
	return zero, false
}

// FreeSlots returns how many slots are free.
func (q *BasicMPMCQueue[T]) FreeSlots() uint64 {
	// Approximate: capacity minus the number of used slots.
	return q.capacity - q.UsedSlots()
}

// UsedSlots returns an approximate count of used slots.
func (q *BasicMPMCQueue[T]) UsedSlots() uint64 {
	enq := atomic.LoadUint64(&q.enqueuePos)
	deq := atomic.LoadUint64(&q.dequeuePos)
	return enq - deq
}

## /pkg/optmpmc_sharded/optmpmc_sharded.go
package optmpmc_sharded

import (
	"math/rand"
	"runtime"
	"sync/atomic"
)

// optCell is the padded slot used by each shard's ring buffer.
type optCell[T any] struct {
	sequence uint64
	value    T
	_pad     [48]byte // pad to roughly 64 bytes per cell
}

// shard holds one ring buffer plus its enqueue/dequeue positions.
type shard[T any] struct {
	_pad0      [8]uint64
	enqueuePos uint64
	_pad1      [7]uint64
	dequeuePos uint64
	_pad2      [7]uint64

	buffer   []optCell[T]
	mask     uint64
	capacity uint64
}

// ShardedOptimizedMPMCQueue is a lock‑free, sharded MPMC queue.
type ShardedOptimizedMPMCQueue[T any] struct {
	shards    []*shard[T]
	numShards uint64
	// totalCapacity is the logical capacity reported by FreeSlots()/UsedSlots().
	totalCapacity uint64
}

// New creates a new sharded queue with the specified total capacity.
// The number of shards is determined by
//
//	var numShards uint64 = uint64(runtime.NumCPU())
//
// and the logical capacity is partitioned among shards so that the sum equals the requested capacity.
// Each shard's internal buffer is allocated with a capacity that is a power‑of‑two.
func New[T any](capacity uint64) *ShardedOptimizedMPMCQueue[T] {
	const minShardCap = 64
	var numShards uint64 = uint64(runtime.NumCPU())
	if capacity < minShardCap {
		numShards = 1
	} else if capacity/minShardCap < numShards {
		numShards = capacity / minShardCap
		if numShards == 0 {
			numShards = 1
		}
	}

	// Partition the logical capacity among shards.
	// Compute the base logical capacity per shard.
	base := capacity / numShards
	// p is the largest power‑of‑two less than or equal to base.
	var p uint64 = 1
	for p*2 <= base {
		p *= 2
	}
	// k shards will receive 2*p slots and the rest will receive p slots.
	k := int(capacity/p) - int(numShards)

	shards := make([]*shard[T], numShards)
	for i := uint64(0); i < numShards; i++ {
		capForShard := p
		if int(i) < k {
			capForShard = 2 * p
		}
		shards[i] = newShard[T](capForShard)
	}

	return &ShardedOptimizedMPMCQueue[T]{
		shards:        shards,
		numShards:     numShards,
		totalCapacity: capacity, // logical capacity exactly as requested
	}
}

// newShard creates one shard with the given capacity (which must be a power‑of‑two).
func newShard[T any](capacity uint64) *shard[T] {
	s := &shard[T]{
		buffer:   make([]optCell[T], capacity),
		mask:     capacity - 1,
		capacity: capacity,
	}
	for i := uint64(0); i < capacity; i++ {
		s.buffer[i].sequence = i
	}
	return s
}

// Enqueue adds a value into one of the shards selected at random.
// It spins within the chosen shard until a slot is available.
func (q *ShardedOptimizedMPMCQueue[T]) Enqueue(val T) {
	for {
		// Use math/rand's per‑thread generator (Go 1.22's math/rand/v2) to pick a shard.
		shardIndex := uint64(rand.Int63()) % q.numShards
		s := q.shards[shardIndex]

		for {
			pos := atomic.LoadUint64(&s.enqueuePos)
			cell := &s.buffer[pos&s.mask]
			seq := atomic.LoadUint64(&cell.sequence)
			if int64(seq)-int64(pos) == 0 {
				if atomic.CompareAndSwapUint64(&s.enqueuePos, pos, pos+1) {
					cell.value = val
					atomic.StoreUint64(&cell.sequence, pos+1)
					return
				}
			} else if int64(seq)-int64(pos) < 0 {
				// This shard appears full; break to try a different shard.
				break
			} else {
				runtime.Gosched()
			}
		}
		runtime.Gosched()
	}
}

// Dequeue removes a value from one of the shards selected at random.
// If the chosen shard is empty and a check shows that all shards are empty,
// it immediately returns with false.
func (q *ShardedOptimizedMPMCQueue[T]) Dequeue() (T, bool) {
	for {
		shardIndex := uint64(rand.Int63()) % q.numShards
		s := q.shards[shardIndex]

		pos := atomic.LoadUint64(&s.dequeuePos)
		cell := &s.buffer[pos&s.mask]
		seq := atomic.LoadUint64(&cell.sequence)
		if int64(seq)-int64(pos+1) == 0 {
			if atomic.CompareAndSwapUint64(&s.dequeuePos, pos, pos+1) {
				ret := cell.value
				atomic.StoreUint64(&cell.sequence, pos+s.capacity)
				return ret, true
			}
		} else {
			if q.checkAllEmpty() {
				break
			}
		}
	}
	var zero T
	return zero, false
}

// FreeSlots returns how many logical slots remain free across all shards.
func (q *ShardedOptimizedMPMCQueue[T]) FreeSlots() uint64 {
	return q.totalCapacity - q.UsedSlots()
}

// UsedSlots returns the total number of used slots across shards.
func (q *ShardedOptimizedMPMCQueue[T]) UsedSlots() uint64 {
	var totalUsed uint64
	for _, s := range q.shards {
		enq := atomic.LoadUint64(&s.enqueuePos)
		deq := atomic.LoadUint64(&s.dequeuePos)
		totalUsed += enq - deq
	}
	return totalUsed
}

// checkAllEmpty returns true if every shard is empty.
func (q *ShardedOptimizedMPMCQueue[T]) checkAllEmpty() bool {
	for _, s := range q.shards {
		if atomic.LoadUint64(&s.enqueuePos) != atomic.LoadUint64(&s.dequeuePos) {
			return false
		}
	}
	return true
}

## /pkg/config/config.go
package config

import "github.com/i5heu/GoQueueBench/pkg/testbench"

// Config is an alias for testbench.Config. This allows other programs to import
// the queue configuration without pulling in the entire testbench package.
type Config = testbench.Config

## /pkg/testbench/testbench.go
package testbench

import (
	"sync"
	"time"

	"github.com/i5heu/GoQueueBench/pkg/queue"
)

// Config defines a test configuration.
type Config struct {
	NumMessages  int
	NumProducers int
	NumConsumers int
}

// RunTest spawns producers and consumers, each calling q.Enqueue/q.Dequeue.
// We use the type constraint queue.MPMCQueue[T] to enforce method signatures.
func RunTest[T any, Q queue.MPMCQueue[T]](
	q Q,
	cfg Config,
	valueGenerator func(int) T,
) time.Duration {

	var prodWg sync.WaitGroup
	prodWg.Add(cfg.NumProducers)
	messagesPerProducer := cfg.NumMessages / cfg.NumProducers
	remainder := cfg.NumMessages % cfg.NumProducers

	start := time.Now()

	// Spawn producers
	for i := 0; i < cfg.NumProducers; i++ {
		count := messagesPerProducer
		if i == cfg.NumProducers-1 {
			count += remainder
		}
		go func(count int) {
			defer prodWg.Done()
			for j := 0; j < count; j++ {
				q.Enqueue(valueGenerator(j))
			}
		}(count)
	}

	// Spawn consumers
	var consWg sync.WaitGroup
	consWg.Add(cfg.NumConsumers)
	messagesPerConsumer := cfg.NumMessages / cfg.NumConsumers
	remConsumer := cfg.NumMessages % cfg.NumConsumers

	for i := 0; i < cfg.NumConsumers; i++ {
		count := messagesPerConsumer
		if i == cfg.NumConsumers-1 {
			count += remConsumer
		}
		go func(count int) {
			defer consWg.Done()
			for j := 0; j < count; j++ {
				for {
					if _, ok := q.Dequeue(); ok {
						break
					}
				}
			}
		}(count)
	}

	// Wait for all goroutines to finish
	prodWg.Wait()
	consWg.Wait()

	return time.Since(start)
}

## /pkg/queue/queue.go
package queue

// MPMCQueue is a *type constraint* that ensures any type Q has
// these methods. We never store Q in a runtime interface—
// we only use MPMCQueue at compile time to ensure matching signatures.
type MPMCQueue[T any] interface {
	// Enqueue adds an element to the queue and blocks if the queue is full.
	Enqueue(T)

	// Dequeue removes and returns the oldest element.
	// If the queue is empty (no element is available), it should return a empty T and false, otherwise true.
	Dequeue() (T, bool)

	// FreeSlots returns how many more elements can be enqueued before the queue is full.
	FreeSlots() uint64

	// UsedSlots returns how many elements are currently queued.
	UsedSlots() uint64
}

// Pointer is a constraint that ensures T is always a pointer type.
type Pointer[T any] interface {
	*T
}

// Compile-time enforcement that T must be a pointer.
func enforcePointer[T any, PT interface{ ~*T }](q MPMCQueue[PT]) {}

## /pkg/buffered/buffered.go
package buffered

// BufferedQueue is a simple implementation using a buffered channel.
type BufferedQueue[T any] struct {
	ch chan T
}

// New returns a new BufferedQueue with the given buffer size.
func New[T any](bufferSize uint64) *BufferedQueue[T] {
	return &BufferedQueue[T]{
		ch: make(chan T, bufferSize),
	}
}

// Enqueue sends a value into the buffered channel. It blocks if the channel is full.
func (q *BufferedQueue[T]) Enqueue(val T) {
	q.ch <- val
}

// Dequeue receives a value from the buffered channel. If empty, it returns a zero value.
func (q *BufferedQueue[T]) Dequeue() (val T, ok bool) {
	select {
	case val = <-q.ch:
		return val, true
	default:
		return val, false
	}
}

// FreeSlots returns how many elements we can enqueue before the channel is full.
func (q *BufferedQueue[T]) FreeSlots() uint64 {
	return uint64(cap(q.ch) - len(q.ch))
}

// UsedSlots returns how many elements are currently in the channel.
func (q *BufferedQueue[T]) UsedSlots() uint64 {
	return uint64(len(q.ch))
}

## /pkg/optmpmc/optmpmc.go
package optmpmc

import (
	"runtime"
	"sync/atomic"
)

// optCell represents one slot in the ring buffer with cache‑line padding.
type optCell[T any] struct {
	sequence uint64
	value    T
	_pad     [48]byte // pad to roughly 64 bytes per cell
}

// OptimizedMPMCQueue is a lock‑free MPMC queue with extra padding to reduce false sharing.
type OptimizedMPMCQueue[T any] struct {
	_pad0      [8]uint64    // padding to avoid false sharing
	enqueuePos uint64       // next enqueue position
	_pad1      [7]uint64    // complete a cache line
	dequeuePos uint64       // next dequeue position
	_pad2      [7]uint64    // additional padding
	buffer     []optCell[T] // ring buffer
	mask       uint64       // fast modulo (capacity - 1)
	capacity   uint64       // total capacity
}

// New creates a new OptimizedMPMCQueue with the given capacity (rounded up to a power of 2).
func New[T any](capacity uint64) *OptimizedMPMCQueue[T] {
	if capacity&(capacity-1) != 0 {
		capPow := uint64(1)
		for capPow < capacity {
			capPow <<= 1
		}
		capacity = capPow
	}
	q := &OptimizedMPMCQueue[T]{
		buffer:   make([]optCell[T], capacity),
		mask:     capacity - 1,
		capacity: capacity,
	}
	for i := uint64(0); i < capacity; i++ {
		q.buffer[i].sequence = i
	}
	return q
}

// Enqueue adds a value into the queue.
// It spins until a slot is available.
func (q *OptimizedMPMCQueue[T]) Enqueue(val T) {
	for {
		pos := atomic.LoadUint64(&q.enqueuePos)
		cell := &q.buffer[pos&q.mask]
		seq := atomic.LoadUint64(&cell.sequence)
		if int64(seq)-int64(pos) == 0 {
			if atomic.CompareAndSwapUint64(&q.enqueuePos, pos, pos+1) {
				cell.value = val
				atomic.StoreUint64(&cell.sequence, pos+1)
				return
			}
		} else {
			runtime.Gosched()
		}
	}
}

// Dequeue removes and returns a value from the queue.
// It spins until a cell is ready.
func (q *OptimizedMPMCQueue[T]) Dequeue() (T, bool) {
	pos := atomic.LoadUint64(&q.dequeuePos)
	cell := &q.buffer[pos&q.mask]
	seq := atomic.LoadUint64(&cell.sequence)
	if int64(seq)-int64(pos+1) == 0 {
		if atomic.CompareAndSwapUint64(&q.dequeuePos, pos, pos+1) {
			ret := cell.value
			atomic.StoreUint64(&cell.sequence, pos+q.capacity)
			return ret, true
		}
	} else {
		runtime.Gosched()
	}

	var zero T
	return zero, false
}

// FreeSlots returns how many slots are free.
func (q *OptimizedMPMCQueue[T]) FreeSlots() uint64 {
	return q.capacity - q.UsedSlots()
}

// UsedSlots returns an approximate count of used slots.
func (q *OptimizedMPMCQueue[T]) UsedSlots() uint64 {
	enq := atomic.LoadUint64(&q.enqueuePos)
	deq := atomic.LoadUint64(&q.dequeuePos)
	return enq - deq
}

